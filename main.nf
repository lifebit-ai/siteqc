#!/usr/bin/env nextflow
/*
========================================================================================
                         siteqc
========================================================================================
siteqc Analysis Pipeline.
#### Homepage / Documentation
----------------------------------------------------------------------------------------
*/

def helpMessage() {
    // TODO : Add to this help message with new command line parameters
    log.info nfcoreHeader()
    log.info"""

    Usage:

    The typical command for running the pipeline is as follows:

    nextflow run  lifebit-ai/siteqc --input ..

    Mandatory arguments:
      --input [file]                  File with list of full paths to bcf files and their indexes.
                                      Bcf files can be compressed but in a readable for bcftools format.
                                      Example:
                                      #-----my_bcf_files_list.csv-----------#
                                      | bcf,index                           |
                                      | <file1.bcf>,<file1.bcf.idx>         |
                                      | <file2.bcf.gz>,<file2.bcf.gz.csi>   |
                                      | <file3.bcf.bgz>,<file3.bcf.bgz.tbx> |
                                      #-------------------------------------#
                                      The name of the files must be consistent across files
                                      and follow a specific pattern:
                                      {name}_{CHR}_{START_POS}_{END_POS}.bcf.gz
                                      Example:
                                      test_all_chunks_merged_norm_chr10_53607810_55447336.bcf.gz
                                      Consistency is important here as a variable ('region')
                                      is extracted from the filename.

      --triofile [file]               File describing the family trios of participants in bcf files.
                                      Input to triodata_define process.
      --included_samples [file]       File with a list of samples' (participants') platekeys to include in analysis..
                                      Input to triodata_define process.
                                      These two arguments can be dropped and triodata_define process will be skipped if
                                      both outputs of this process are provided by user with arguments --triodata_keep_pheno
                                      and --triodata_fam (see optional arguments).

      --xx_sample_ids [file]          Path to file that contains the XX sample ids. Must be single column, without a header.
      --xy_sample_ids [file]          Path to file that contains the XY sample ids. Must be single column, without a header.
                                      This is important and required information because X chromosomes from female and male 
                                      participants are treated differently. X chromosome metrics will be calculaated only
                                      using data from participants with a determined sex.
    Optional arguments:
      --updfile [file]                File with a list of participants' platekeys to be exluded from the analysis.
                                      Optional input to triodata_define process.

      --triodata_keep_pheno [file]    File with a list of participants to keep in analysis.
      --triodata_fam [file]           A .fam file describing families of participants selected to keep in analysis.
                                      If both triodata files above are provided - triodata_define process is skipped and
                                      these files are used instead.

      --mend_err_p3_keep_fam [file]   A .fam file of families that should be kept after mend_err_* filtering processes.
                                      By default such file is generated by mend_dist process, but user can provide a
                                      different file to be used instead. In this case mend_dist and mend_err_p2 processes
                                      will be skipped.


    Other options:
      -profile [str]                  Configuration profile to use. Can use multiple (comma separated)
                                      Available: standard (default), test (= test1), test1, test2, ... test4.
      --outdir [file]                 The output directory where the results will be saved
      --publish_dir_mode [str]        Mode for publishing results in the output directory. Available: symlink, rellink, link, copy, copyNoFollow, move (Default: copy)
      --email [email]                 Set this parameter to your e-mail address to get a summary e-mail with details of the run sent to you when the workflow exits
      --email_on_fail [email]         Same as --email, except only send mail if the workflow is not successful
      --max_multiqc_email_size [str]  Threshold size for MultiQC report to be attached in notification email. If file generated by pipeline exceeds the threshold, it will not be attached (Default: 25MB)
      -name [str]                     Name for the pipeline run. If not specified, Nextflow will automatically generate a random mnemonic

    AWSBatch options:
      --awsqueue [str]                The AWSBatch JobQueue that needs to be set when running on AWSBatch
      --awsregion [str]               The AWS Region for your AWS Batch job to run on
      --awscli [str]                  Path to the AWS CLI tool
    """.stripIndent()
}

// Show help message
if (params.help) {
    helpMessage()
    exit 0
}

// Define variables
query_format_start = params.query_format_start
query_format_miss1 = params.query_format_miss1
query_include_miss1 = params.query_include_miss1
query_format_miss2 = params.query_format_miss2
query_exclude_miss2 = params.query_exclude_miss2
awk_expr_miss2 = params.awk_expr_miss2
query_format_med_cov_all = params.query_format_med_cov_all
awk_expr_med_cov_all = params.awk_expr_med_cov_all
query_format_med_cov_nonmisss = params.query_format_med_cov_nonmisss
query_exclude_med_cov_nonmiss =  params.query_exclude_med_cov_nonmiss
awk_expr_med_cov_nonmiss = params.awk_expr_med_cov_nonmiss
query_format_median_gq = params.query_format_median_gq
query_exclude_median_gq = params.query_exclude_median_gq
awk_expr_median_gq = params.awk_expr_median_gq
query_format_ab_ratio_p1 = params.query_format_ab_ratio_p1
query_include_ab_ratio_1 = params.query_include_ab_ratio_1
query_format_ab_ratio_p2 = params.query_format_ab_ratio_p2
query_include_ab_ratio_2 =  params.query_include_ab_ratio_2
query_format_pull_ac = params.query_format_pull_ac
awk_expr_miss1 = params.awk_expr_miss1
awk_expr_miss2 = params.awk_expr_miss2
awk_expr_ab_ratio_1 = params.awk_expr_ab_ratio_1

/*
 * Check all important required inputs
 */

// Check if user provided input csv file containing paths to bcf files and their indexes
if (!params.input) exit 1, "The list of input bcf/vcf files was not provided. \nPlease specify a csv file containing paths to bcf/vcf files and their indexes with --input [file] option. \nUse --help option for more information."

// Check if user provided triodata file or files that should be generated from it by the pipeline.
if (!params.triofile && (params.triodata_keep_pheno == params.empty_file || params.triodata_fam == params.empty_file) ) exit 1, "The list of family trios was not provided. \nPlease specify a triodata file with --triofile [file] option.\nAlternatively, you can provide ready to use .fam and .keep files with parameters\n--triodata_fam [file] and --triodata_keep_pheno [file].\nUse --help option for more information."

// Check if user provided lists of male and female participants (important for handling X chromosome differently for males and females)
if (!params.xx_sample_ids) exit 1, "The list of female participants was not provided.\nPlease specify the file with --xx_sample_ids [file] option.\nThis is important and required information because X chromosomes from female and male participants are treated differently.\nX chromosome metrics will be calculaated only using data from participants with a determined sex.\nUse --help option for more information."

if (!params.xy_sample_ids) exit 1, "The list of male participants was not provided.\nPlease specify the file with --xy_sample_ids [file] option.\nThis is important and required information because X chromosomes from female and male participants are treated differently.\nX chromosome metrics will be calculaated only using data from participants with a determined sex.\nUse --help option for more information."


// Define channels based on params
if (params.input.endsWith(".csv")) {

  Channel.fromPath(params.input)
                        .ifEmpty { exit 1, "Input .csv list of input tissues not found at ${params.input}. Is the file path correct?" }
                        .splitCsv(sep: ',',  skip: 1)
                        .map { bcf, index -> ['chr'+file(bcf).simpleName.split('_chr').last() , file(bcf), file(index)] }
                        // Filter out chunks from chrY and chrM - they should not be analysed in SiteQC pipeline
                        .filter { it[0] =~ /chr[^YM]/ }
                        .into { ch_bcfs_start_file;
                                ch_bcfs_miss1;
                                ch_bcfs_miss2;
                                ch_bcfs_complete_sites;
                                ch_bcfs_med_cov_all;
                                ch_bcfs_med_cov_non_miss;
                                ch_bcfs_median_gq;
                                ch_bcfs_ab_ratio_p1;
                                ch_bcfs_ab_ratio_p2;
                                ch_bcfs_pull_ac;
                                ch_bcfs_mend_err_p1 }

  Channel.fromPath(params.input)
         .ifEmpty { exit 1, "Input .csv list of input tissues not found at ${params.input}. Is the file path correct?" }
         .splitCsv(sep: ',',  skip: 1)
         .map { bcf, index -> ['chr'+file(bcf).simpleName.split('_chr').last().split('_').first(),
                               file("${params.s3_path_1kg_start}" + 'chr'+file(bcf).simpleName.split('_chr').last().split('_').first() + "${params.s3_path_1kg_end}") ] }
         // Do not use 1kg files chunks from chrY  and chrM  - they should not be analysed in SiteQC pipeline
         // And chrX - as 1000genomes files are only used in annotation step, from which chrX is excluded.
         .filter { it[0] =~ /chr[^XYM]/ }
         .set { ch_1kg_archives }

}


// List with sample ids to include/exclude
Channel.fromPath(params.xy_sample_ids, checkIfExists: true)
                    .into { ch_xy_sample_id_start_file;
                            ch_xy_sample_id_miss1;
                            ch_xy_sample_id_miss2;
                            ch_xy_sample_id_med_cov_all;
                            ch_xy_sample_id_med_cov_non_miss;
                            ch_xy_sample_id_median_gq;
                            ch_xy_sample_id_ab_ratio_p1;
                            ch_xy_sample_id_ab_ratio_p2;
                            ch_xy_sample_id_complete_sites }
Channel.fromPath(params.xx_sample_ids, checkIfExists: true)
                    .into { ch_xx_sample_id_start_file;
                            ch_xx_sample_id_miss1;
                            ch_xx_sample_id_miss2;
                            ch_xx_sample_id_med_cov_all;
                            ch_xx_sample_id_med_cov_non_miss;
                            ch_xx_sample_id_median_gq;
                            ch_xx_sample_id_ab_ratio_p1;
                            ch_xx_sample_id_ab_ratio_p2;
                            ch_xx_sample_id_complete_sites }


// Header log info
log.info nfcoreHeader()
def summary = [:]
if (workflow.revision) summary['Pipeline Release'] = workflow.revision
// TODO nf-core: Report custom parameters here
summary['Max Resources']    = "$params.max_memory memory, $params.max_cpus cpus, $params.max_time time per job"
if (workflow.containerEngine) summary['Container'] = "$workflow.containerEngine - $workflow.container"
summary['Output dir']       = params.outdir
summary['Launch dir']       = workflow.launchDir
summary['Working dir']      = workflow.workDir
summary['Script dir']       = workflow.projectDir
summary['User']             = workflow.userName
summary['Config Profile'] = workflow.profile
if (params.config_profile_description) summary['Config Profile Description'] = params.config_profile_description
if (params.config_profile_contact)     summary['Config Profile Contact']     = params.config_profile_contact
if (params.config_profile_url)         summary['Config Profile URL']         = params.config_profile_url
summary['Config Files'] = workflow.configFiles.join(', ')
log.info summary.collect { k,v -> "${k.padRight(18)}: $v" }.join("\n")
log.info "-\033[2m--------------------------------------------------\033[0m-"



// CONDITION: if triodata_fam or triodata_keep_pheno files are not provided -
// run triodata_define process to generate them.

if (params.triodata_keep_pheno == params.empty_file || params.triodata_fam == params.empty_file) {

  if (params.triodata_keep_pheno == params.empty_file) {
    log.warn "Trio file of families to keep was not provided. (You can specify it with --triodata_keep_pheno argument)"
  }
  if (params.triodata_fam == params.empty_file) {
    log.warn "Trio .fam file of families was not provided. (You can specify it with --triodata_fam argument)"
  }
  log.warn "Triodata_define process will be run because either of triodata_keep_pheno or triodata_fam files was not provided."

  Channel
      .fromPath(params.triofile)
      .set {ch_triofile}

  Channel
      .fromPath(params.included_samples)
      .set {ch_included_samples}

  Channel
      .fromPath(params.updfile)
      .set {ch_updfile}

  /*
   * STEP - triodata_define:  Create triodata files if not provided by user
   */
  process triodata_define {
      publishDir "${params.outdir}/triodata/", mode: params.publish_dir_mode

      input:
      file(triofile) from ch_triofile
      file(included_samples) from ch_included_samples
      file(updfile) from ch_updfile

      output:
      file("triodata.fam") into ch_triodata_fam
      file("keep.txt") into ch_triodata_keep_pheno
      script:
      """
      trio_define.R ${triofile} ${included_samples} ${updfile} "triodata.fam" "keep.txt"
      """
  }

}

if (params.triodata_keep_pheno != params.empty_file && params.triodata_fam != params.empty_file) {

  Channel
    .fromPath(params.triodata_keep_pheno)
    .set {ch_triodata_keep_pheno}

  Channel
    .fromPath(params.triodata_fam)
    .set {ch_triodata_fam}

}
//END OF CONDIION

(ch_mend_err_p2_fam,
 ch_mend_err_p3_fam) = ch_triodata_fam.into(2)


/*
 * STEP - start_file:  Create a backbone of IDs for other data to be joined to
 */
// TODO redefine logic of "if "$sexChrom", capture substring from basename?
// #Sample lists for XX/XY QC
//xx='xx_females_illumina_ploidy_samples_40740.tsv'
//xy='xy_males_illumina_ploidy_samples_35924.tsv'
// bcftools query -S, --samples-file <file>         file of samples to include
// head results/startfiles/start_file_chr10_52955340_55447336 
// chr10 52955340 A G
// chr10 52955649 T C
process start_file {
    publishDir "${params.outdir}/startfiles/", mode: params.publish_dir_mode

    input:
    set val(region), file(bcf), file(index) from ch_bcfs_start_file
    each file(xy_sample_ids) from ch_xy_sample_id_start_file
    each file(xx_sample_ids) from ch_xx_sample_id_start_file

    output:
    tuple val(region), file("start_file_*") into ch_template_startfiles
    // head results/startfiles/start_file_chr10_52955340_55447336 
    // chr10 52955340 A G
    // chr10 52955649 T C

    script:
    """
    if [[ $region == *"chrX"* ]]; then
      bcftools query -f '${query_format_start}' ${bcf} -o start_file_${region}_XY -S ${xy_sample_ids}
      bcftools query -f '${query_format_start}' ${bcf} -o start_file_${region}_XX -S ${xx_sample_ids}
    else
      bcftools query -f '${query_format_start}' ${bcf} -o start_file_${region}
    fi
    """
  }

 /*
 * STEP - missingness_1: Count fully missing GTs
 */
process missingness_1 {
    publishDir "${params.outdir}/missing1/", mode: params.publish_dir_mode

    input:
    set val(region), file(bcf), file(index) from ch_bcfs_miss1
    each file(xy_sample_ids) from ch_xy_sample_id_miss1
    each file(xx_sample_ids) from ch_xx_sample_id_miss1

    output:
    tuple val(region), file("missing1_*") into ch_outputs_miss1

    script:
    """
    if [[ $region == *"chrX"* ]]; then
        bcftools query ${bcf} -f '${query_format_miss1}' -i '${query_include_miss1}' -S ${xy_sample_ids} | awk '${awk_expr_miss1}' > missing1_${region}_XY
        bcftools query ${bcf} -f '${query_format_miss1}' -i '${query_include_miss1}' -S ${xy_sample_ids} | awk '${awk_expr_miss1}' > missing1_${region}_XY
    else
        bcftools query ${bcf} -f '${query_format_miss1}' -i '${query_include_miss1}' | awk '${awk_expr_miss1}' > missing1_${region}
    fi
    """
 }

 /*
 * STEP - missingness_2: Count complete GTs only
 */
process missingness_2 {
    publishDir "${params.outdir}/missing2/", mode: params.publish_dir_mode

    input:
    set val(region), file(bcf), file(index) from ch_bcfs_miss2
    each file(xy_sample_ids) from ch_xy_sample_id_miss2
    each file(xx_sample_ids) from ch_xx_sample_id_miss2

    output:
    tuple val(region), file("missing2_*") into ch_outputs_miss2

    script:
    """
    if [[ $region == *"chrX"* ]]; then
        bcftools query ${bcf} -f '${query_format_miss2}' -e '${query_exclude_miss2}' -S ${xy_sample_ids} | awk '${awk_expr_miss2}' > missing2_${region}_XY
        bcftools query ${bcf} -f '${query_format_miss2}' -e '${query_exclude_miss2}' -S ${xx_sample_ids} | awk '${awk_expr_miss2}' > missing2_${region}_XX
    else
        bcftools query ${bcf} -f '${query_format_miss2}' -e '${query_exclude_miss2}' | awk '${awk_expr_miss2}' > missing2_${region}
    fi
    """
 }

 /*
 * STEP - complete_sites: Count the number of participants in autosomal bcfs, and also count separately female and male participants for chrX
 */

// To count participants we need only one autosomal bcf, but additionally also one bcf for chrX.
// For this I fork general bcf input channel into autosomal bcfs and chrX bcfs, randomly select one bcf from each branch,
// and combine two bcfs back in one channel.
ch_bcfs_complete_sites
        .branch {
            autosomes: it[0] =~ /chr[^X]/
            chrX: it[0] =~ /chrX/
        }
        .set { ch_bcfs_split }

ch_bcf_1_autos_1_X = ch_bcfs_split.autosomes.randomSample(1)
                     .mix(ch_bcfs_split.chrX.randomSample(1))

process complete_sites {
    publishDir "${params.outdir}/", mode: params.publish_dir_mode

    input:
    set val(region), file(bcf), file(index) from ch_bcf_1_autos_1_X
    each file(xy_sample_ids) from ch_xy_sample_id_complete_sites
    each file(xx_sample_ids) from ch_xx_sample_id_complete_sites

    output:
    set val(region), file("*N_samples*") into ch_n_samples_files

    script:
    """
    if [[ $region == *"chrX"* ]]; then
        bcftools query -l ${bcf} -S ${xy_sample_ids} | wc -l > sex_chr_N_samples_XY
        bcftools query -l ${bcf} -S ${xx_sample_ids} | wc -l > sex_chr_N_samples_XX
    else
        bcftools query -l ${bcf} | wc -l > autosomes_N_samples
    fi
    """
 }

 /*
 * STEP - median_coverage_all: Produce median value for depth across all GT
 */

 process median_coverage_all {
     publishDir "${params.outdir}/median_coverage_all/", mode: params.publish_dir_mode

     input:
     set val(region), file(bcf), file(index) from ch_bcfs_med_cov_all
     each file(xy_sample_ids) from ch_xy_sample_id_med_cov_all
     each file(xx_sample_ids) from ch_xx_sample_id_med_cov_all

     output:
     tuple val(region),file("medianCoverageAll*") into ch_outputs_med_cov_all

     script:
     """
     if [[ $region == *"chrX"* ]]; then
         bcftools query ${bcf} -f '${query_format_med_cov_all}' -S ${xy_sample_ids} | awk '${awk_expr_med_cov_all}' > medianCoverageAll${region}_XY
         bcftools query ${bcf} -f '${query_format_med_cov_all}' -S ${xy_sample_ids} | awk '${awk_expr_med_cov_all}' > medianCoverageAll${region}_XX 
     else
         bcftools query ${bcf} -f '${query_format_med_cov_all}'| awk '${awk_expr_med_cov_all}' > medianCoverageAll${region}
     fi
     """
  }

//  /*
//  * STEP - median_coverage_non_miss: Median coverage for fully present genotypes
//  */

 process median_coverage_non_miss {
     publishDir "${params.outdir}/medianCoverageNonMiss/", mode: params.publish_dir_mode

     input:
     set val(region), file(bcf), file(index) from ch_bcfs_med_cov_non_miss
     each file(xy_sample_ids) from ch_xy_sample_id_med_cov_non_miss
     each file(xx_sample_ids) from ch_xx_sample_id_med_cov_non_miss

     output:
     tuple val(region), file("medianNonMiss_depth_*") into ch_outputs_med_cov_nonmiss

     script:

     """
     if [[ $region == *"chrX"* ]]; then
         bcftools query ${bcf} -f '${query_format_med_cov_nonmisss}' -e '${query_exclude_med_cov_nonmiss}' -S ${xy_sample_ids} | awk '${awk_expr_med_cov_nonmiss}' > medianNonMiss_depth_${region}_XY
         bcftools query ${bcf} -f '${query_format_med_cov_nonmisss}' -e '${query_exclude_med_cov_nonmiss}' -S ${xx_sample_ids} | awk '${awk_expr_med_cov_nonmiss}' > medianNonMiss_depth_${region}_XX
     else
         bcftools query ${bcf} -f '${query_format_med_cov_nonmisss}' -e '${query_exclude_med_cov_nonmiss}' | awk '${awk_expr_med_cov_nonmiss}' > medianNonMiss_depth_${region}
     fi
     """
  }

//  /*
//  * STEP - median_gq: Calculate median GQ
//  */
 process median_gq {
     publishDir "${params.outdir}/medianGQ", mode: params.publish_dir_mode

     input:
     set val(region), file(bcf), file(index) from ch_bcfs_median_gq
     each file(xy_sample_ids) from ch_xy_sample_id_median_gq
     each file(xx_sample_ids) from ch_xx_sample_id_median_gq

     output:
     tuple val(region), file("medianGQ_*") into ch_outputs_median_gq

     script:
     """
     if [[ $region == *"chrX"* ]]; then
         bcftools query ${bcf} -f '${query_format_median_gq}' -e '${query_exclude_median_gq}' -S ${xy_sample_ids} | awk '${awk_expr_median_gq}'  > medianGQ_${region}_XY
         bcftools query ${bcf} -f '${query_format_median_gq}' -e '${query_exclude_median_gq}' -S ${xx_sample_ids} | awk '${awk_expr_median_gq}'  > medianGQ_${region}_XX
     else
         bcftools query ${bcf} -f '${query_format_median_gq}' -e '${query_exclude_median_gq}' | awk '${awk_expr_median_gq}'  > medianGQ_${region}
     fi
     """
  }

//  /*
//  * STEP - ab_ratio_p1: AB ratio calculation - number of hets passing binomial test (reads supporting het call)
//  */

 process ab_ratio_p1 {
     publishDir "${params.outdir}/AB_hetPass/", mode: params.publish_dir_mode

     input:
     set val(region), file(bcf), file(index) from ch_bcfs_ab_ratio_p1
     each file(xy_sample_ids) from ch_xy_sample_id_ab_ratio_p1
     each file(xx_sample_ids) from ch_xx_sample_id_ab_ratio_p1

     output:
     tuple val(region), file("hetPass_*") into ch_outputs_ab_ratio_p1

     script:

     """
     #We only calculate AB ratio for XX
     if [[ $region == *"chrX"* ]]; then
         bcftools query ${bcf} -f '${query_format_ab_ratio_p1}' -S ${xx_sample_ids} -i '${query_include_ab_ratio_1}' | awk '${awk_expr_ab_ratio_1}' > hetPass_${region}_XX
     else
         bcftools query ${bcf} -f '${query_format_ab_ratio_p1}' -i '${query_include_ab_ratio_1}' | awk '${awk_expr_ab_ratio_1}' > hetPass_${region}
     fi
     """
  }

//  /*
//  * STEP - ab_ratio_p2: Number of het GTs for p2 AB ratio
//  */




process ab_ratio_p2 {
    publishDir "${params.outdir}/AB_hetAll", mode: params.publish_dir_mode

    input:
    set val(region), file(bcf), file(index) from ch_bcfs_ab_ratio_p2
    each file(xy_sample_ids) from ch_xy_sample_id_ab_ratio_p2
    each file(xx_sample_ids) from ch_xx_sample_id_ab_ratio_p2

    output:
    tuple val(region), file("hetAll_*") into ch_outputs_ab_ratio_p2

    script:
    """
    # We only calculate AB ratio for XX
    if [[ $region == *"chrX"* ]]; then
        bcftools query ${bcf} -f '${query_format_ab_ratio_p2}' -S ${xx_sample_ids} -i '${query_include_ab_ratio_2}' | awk '${awk_expr_ab_ratio_1}' > hetAll_${region}_XX
    else
        bcftools query ${bcf} -f '${query_format_ab_ratio_p2}' -i '${query_include_ab_ratio_2}' | awk '${awk_expr_ab_ratio_1}' > hetAll_${region}
    fi
    """
 }

 /*
 * STEP - pull_ac: Pull AC from all files and store for addition to site metrics
 */
// TODO: What subfolder do we need to store the files in?
process pull_ac {
    publishDir "${params.outdir}/AC_counts/", mode: params.publish_dir_mode

    input:
    set val(region), file(bcf), file(index) from ch_bcfs_pull_ac
    
    output:
    tuple val(region), file("*_AC") into ch_outputs_pull_ac

    script:
    """
    bcftools query ${bcf} -f '${query_format_pull_ac}' > ${region}_AC
    """
 }

/*
 * STEP - pull_1kg_p3_sites: Pull sites from 1000KGP3
 */
// TOCHECK: Ok to run once and provide as a resource?
process pull_1kg_p3_sites {
    publishDir "${params.outdir}/pull_1000G_files/", mode: params.publish_dir_mode

    input:
    tuple val(chr), file(vcf_archive) from ch_1kg_archives

    output:
    tuple val(chr), file("tmp_1kgp_*.txt") into ch_1kg_sites

    script:

    """
    zcat ${vcf_archive} | awk '${params.awk_expr_pull_1kg_p3_sites}'  > tmp_1kgp_${chr}.txt
    """
}

  /*
  * STEP - mend_err_p1: Create a bed file for the mendel error calcs
  */
 process mend_err_p1 {
     publishDir "${params.outdir}/MendelErr/", mode: params.publish_dir_mode

     input:
     set val(region), file(bcf), file(index) from ch_bcfs_mend_err_p1 
     each file(triodata_keep_file) from ch_triodata_keep_pheno

     output:
     tuple val(region),
           file("BED_trio_*.bed"),
           file("BED_trio_*.bim"),
           file("BED_trio_*.fam"),
           file("BED_trio_*.log") into ch_mend_err_p1_plink_files_p2,
                                       ch_mend_err_p1_plink_files_p3


     script:
     """
     plink2 --bcf ${bcf} \
     --make-bed \
     --set-missing-var-ids ${params.mend_err_p1_rset_missing_var_ids} \
     --vcf-half-call ${params.mend_err_p1_vcf_half_call} \
     --new-id-max-allele-len ${params.mend_err_p1_new_id_max_allele_len} \
     --double-id \
     --real-ref-alleles \
     --allow-extra-chr \
     --out BED_trio_${region} \
     --keep ${triodata_keep_file}
     """
 }

// CONDITION: if file for mend_err_p3_keep_fam is not provided, run these two processes:
if (params.mend_err_p3_keep_fam == params.empty_file) {

  log.warn "File specifying families to keep with mend_err_p3 process was not provided. Processes mend_err_p2 and mend_dist will be run for each input bcf/vcf file to determine families to keep that fulfill the 4SD criteria. If you wish to manually specify which families to keep, provide a .fam file with argument --mend_err_p3_keep_fam ."

/*
 * STEP - mend_err_p2: Calculate mendelian errors
 */

process mend_err_p2 {
    publishDir "${params.outdir}/MendelErr/", mode: params.publish_dir_mode

    input:
    each file(predefined_fam) from ch_mend_err_p2_fam
    tuple val(region), file(bed), file(bim), file(fam), file(log) from ch_mend_err_p1_plink_files_p2.filter { it[0] =~ /chr[^X]/ } //excluding chrX because it can't be used for mend_dist

    output:
    file "*.fmendel" into ch_mend_err_p2_plink_files

    script:

    """
    plink --bed ${bed} \
    --bim ${bim} \
    --fam ${predefined_fam} \
    --allow-extra-chr \
    --allow-no-sex \
    --mendel summaries-only \
    --out MendErr_${region}
    """
}

/*
 * STEP - mend_dist: Summary stats and good families for Mendel errors
 */
process mend_dist {
    publishDir "${params.outdir}/Mend_dist/", mode: params.publish_dir_mode

    input:
    file (fmendel) from ch_mend_err_p2_plink_files.collect()

    output:
    file "FamilyWise.summarystats" into ch_mend_dist_out
    file "MendelFamilies_4SD.fam" into ch_mend_dist_keep_families
    file "SD_mendel_family.png" into ch_mend_dist_out_plot

    script:

    """
    mendelerror_family_plotting.R .
    """
}

}


/*
 * STEP - mend_err_p3: Calculate mendel errors on just good families
 */

// Based on if keep-pheno file was provided by user, select appropriate channel
ch_mend_err_p3_keep_fam = params.mend_err_p3_keep_fam != params.empty_file ? Channel.fromPath("$params.mend_err_p3_keep_fam") : ch_mend_dist_keep_families

process mend_err_p3 {
    publishDir "${params.outdir}/MendelErrSites", mode: params.publish_dir_mode

    input:
    each file(predefined_fam) from ch_mend_err_p3_fam
    tuple val(region), file(bed), file(bim), file(fam), file(log) from ch_mend_err_p1_plink_files_p3
    each file(keep_fam) from ch_mend_err_p3_keep_fam

    output:
    tuple val(region), file("MendErr*.lmendel") into ch_mend_err_p3_out

    script:

    """
    plink --bed ${bed} \
    --bim ${bim} \
    --fam ${predefined_fam} \
    --allow-extra-chr \
    --allow-no-sex \
    --keep-fam ${keep_fam} \
    --mendel summaries-only \
    --out MendErr_${region}
    """
}

// NOTE: (Daniel's note) Just before this step is where we want a checklist, that all chunks are completed

/*
* STEP - aggregate_annotation:  Annotate and make pass/fail. If king set to T in env, print subset of cols
*/


// Creating a joined tuple with all files that are needed for aggregate annotation step per input bcf file.

// All channels (except 1kg) are joined by the very first (0-th) item in tuple, that stores "region" value.
// This ensures that only files generated from same input bcf file are aggregated together.
// A file from 1000G ch is added by mapping by chr value, not region. To generate chr value from first
// channel in the chain - startfiles channel - the chr value is generated with .map operator from region value.
ch_joined_outputs_to_aggregate =
   ch_template_startfiles
         .map { region, startfile -> [region.split('_').first(), region, startfile] }
         .join(ch_1kg_sites)
         .map { chr,region,startfile,i1kg -> [region, startfile, i1kg]}
         .join(ch_outputs_miss1)
         .join(ch_outputs_miss2)
         .join(ch_outputs_med_cov_all)
         .join(ch_outputs_med_cov_nonmiss)
         .join(ch_outputs_median_gq)
         .join(ch_outputs_ab_ratio_p2)
         .join(ch_outputs_ab_ratio_p1)
         .join(ch_mend_err_p3_out)
         .join(ch_outputs_pull_ac)
         //excluding chrX files because it doesn't need to go to Ancestry and relatedness pipeline.
         .filter { it[0] =~ /chr[^X]/ }
         //There will be only 1 N_samples file for all autosomes, but we have to include it in each bcf tuple.
         //.combine() operator makes sure each autosomal bcf tuple gets it,
         //.filter() removes two chrX N_samples count files that we don't need here.
         //.map() removes first element of tuple for n_samples channel that is region value that we used to filter only autosomal n_counts file.
         .combine(ch_n_samples_files
                    .filter { it[0] =~ /chr[^X]/ }
                    .map { region, n_file -> [n_file]}
         )
         .view()



process aggregate_annotation {
    publishDir "${params.outdir}/Annotation/", mode: params.publish_dir_mode

    input:
    tuple val(region),
          file(startfile),
          file(one_thousand_g_file),
          file(miss1),
          file(miss2),
          file(medianCoverageAll),
          file(medianNonMiss),
          file(medianGQ),
          file(hetAll),
          file(hetPass),
          file(MendErr),
          file(AC_counts),
          file(N_samples) from ch_joined_outputs_to_aggregate

    output:
    file "BCFtools_site_metrics_*.txt" into ch_aggr_annotation_1
    file "Summary_stats/*_all_flags.txt" into ch_summary_stats
    script:
    """
    mkdir -p "Summary_stats"
    annotatePerChunk.R \
    ${region} \
    ${startfile} \
    ${miss1} \
    ${miss2} \
    ${medianCoverageAll} \
    ${medianNonMiss} \
    ${medianGQ} \
    ${hetAll} \
    ${hetPass} \
    ${MendErr} \
    '.' \
    ${N_samples} \
    ${AC_counts} \
    ${one_thousand_g_file} \
    ${params.king}
    """
}



def nfcoreHeader() {
    // Log colors ANSI codes
    c_black = params.monochrome_logs ? '' : "\033[0;30m";
    c_blue = params.monochrome_logs ? '' : "\033[0;34m";
    c_cyan = params.monochrome_logs ? '' : "\033[0;36m";
    c_dim = params.monochrome_logs ? '' : "\033[2m";
    c_green = params.monochrome_logs ? '' : "\033[0;32m";
    c_purple = params.monochrome_logs ? '' : "\033[0;35m";
    c_reset = params.monochrome_logs ? '' : "\033[0m";
    c_white = params.monochrome_logs ? '' : "\033[0;37m";
    c_yellow = params.monochrome_logs ? '' : "\033[0;33m";

    return """
    """.stripIndent()
}

