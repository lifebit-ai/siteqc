/*
 * -------------------------------------------------
 *  Nextflow config file for running tests
 * -------------------------------------------------
 * Defines bundled input files and everything required
 * to run a fast and simple test. Use as follows:
 *   nextflow run nf-core/siteqc -profile test,<docker/singularity>
 */

process {

  // TODO nf-core: Check the defaults for all processes
  cpus = { check_max( 1 * task.attempt, 'cpus' ) }
  memory = { check_max( 7.GB * task.attempt, 'memory' ) }
  time = { check_max( 4.h * task.attempt, 'time' ) }
  container = 'lifebitai/siteqc:1.0dev'
  errorStrategy = { task.exitStatus in [143,137,104,134,139] ? 'retry' : 'ignore' }
}

docker.enabled = true


params {
  config_profile_name = 'Test profile'
  config_profile_description = 'Minimal test dataset to check pipeline function'
  // Limit resources so that this can run on GitHub Actions
  max_cpus = 2
  max_memory = 6.GB
  max_time = 48.h

  // Workflow flags
  outdir = './results'
  publish_dir_mode = 'copy'

  // Files
  empty_file = 's3://lifebit-featured-datasets/projects/gel/siteqc/nodata'

  input = 's3://lifebit-featured-datasets/projects/gel/siteqc/input.csv'
  xx_sample_ids = 's3://lifebit-featured-datasets/projects/gel/siteqc/xx.txt'
  xy_sample_ids = 's3://lifebit-featured-datasets/projects/gel/siteqc/xy.txt'
  triofile = false
  included_samples = 's3://lifebit-featured-datasets/projects/gel/siteqc/sampleList.txt'
  updfile = empty_file
  triodata_keep_pheno = 's3://lifebit-featured-datasets/projects/gel/siteqc/keep.txt'
  triodata_fam = 's3://lifebit-featured-datasets/projects/gel/siteqc/example.fam'
  s3_path_1kg_start = 's3://1000genomes/release/20130502/ALL.'
  s3_path_1kg_end = '.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz'
  mend_err_p3_keep_fam = empty_file

  // Values
  king='T'
  query_format_start = '%CHROM %POS %REF %ALT %INFO/OLD_CLUMPED-%INFO/OLD_MULTIALLELIC\n'
  query_format_miss1 = '%CHROM:%POS-%REF/%ALT-%INFO/OLD_CLUMPED-%INFO/OLD_MULTIALLELIC [%GT ] \n'
  query_include_miss1 = 'GT="./." & FORMAT/DP=0'
  query_format_miss2 = '%CHROM:%POS-%REF/%ALT-%INFO/OLD_CLUMPED-%INFO/OLD_MULTIALLELIC [%GT ] \n'
  query_exclude_miss2 = 'GT~"\\."'
  query_exclude_median_gq = 'GT~"\\."'
  query_exclude_med_cov_nonmiss =  'GT~"\\."'
  query_format_med_cov_all = '%CHROM:%POS-%REF/%ALT-%INFO/OLD_CLUMPED-%INFO/OLD_MULTIALLELIC [ %DP]\n'
  awk_expr_med_cov_all = '{sum=0; n=split(\$0,a)-1; for(i=2;i<=n;i++) sum+=a[i]; asort(a); median=n%2?a[n/2+1]:(a[n/2]+a[n/2+1])/2; print \$1"\t"median}'
  query_format_med_cov_nonmisss = '%CHROM:%POS-%REF/%ALT-%INFO/OLD_CLUMPED-%INFO/OLD_MULTIALLELIC [ %DP]\n'
  awk_expr_med_cov_nonmiss = '{sum=0; n=split(\$0,a)-1; for(i=2;i<=n;i++) sum+=a[i]; asort(a); median=n%2?a[n/2+1]:(a[n/2]+a[n/2+1])/2; print \$1"\t"median}'
  query_format_median_gq = '%CHROM:%POS-%REF/%ALT-%INFO/OLD_CLUMPED-%INFO/OLD_MULTIALLELIC [%GQ ] \n'
  awk_expr_median_gq = '{sum=0; n=split(\$0,a)-1; for(i=2;i<=n;i++) sum+=a[i]; asort(a); median=n%2?a[n/2+1]:(a[n/2]+a[n/2+1])/2; if(median > 99) {median=99}; print \$1"\t"median}'
  query_format_ab_ratio_p1 = '%CHROM:%POS-%REF/%ALT-%INFO/OLD_CLUMPED-%INFO/OLD_MULTIALLELIC [%AD ] \n'
  query_include_ab_ratio_1 = 'GT="het" & binom(FMT/AD) > 0.01'
  query_format_ab_ratio_p2 = '%CHROM:%POS-%REF/%ALT-%INFO/OLD_CLUMPED-%INFO/OLD_MULTIALLELIC [%AD ] \n'
  query_include_ab_ratio_2 =  'GT="het"'
  query_format_pull_ac = '%CHROM:%POS-%REF/%ALT-%INFO/OLD_CLUMPED-%INFO/OLD_MULTIALLELIC %INFO/AC \n'
  awk_expr_miss1 = 'BEGIN{FS=" "} {print $1" "NF-1}'
  awk_expr_miss2 = 'BEGIN{FS=" "} {print $1" "NF-1}'
  awk_expr_ab_ratio_1 = '{print $1"\t"NF -1}'
  awk_expr_pull_1kg_p3_sites = '/^##/ {next} { print $1"\t"$2"\t"$4"\t"$5}'
  mend_err_p1_rset_missing_var_ids = '@:#,\\\$r,\\\$a'
  mend_err_p1_vcf_half_call = 'm'
  mend_err_p1_new_id_max_allele_len = '60 missing'

}


